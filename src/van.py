import jittor as jt
import jittor.nn as nn
from jittor import init
import math
from itertools import repeat
import collections.abc
import warnings
from scipy.special import erfinv
import sys
from collections import defaultdict
from .ham_head import LightHamHead
from .ops import resize

_module_to_models = defaultdict(set)  # dict of sets to check membership of model in module
_model_to_module = {}  # mapping of model names to module names
_model_entrypoints = {}  # mapping of model names to entrypoint fns
_model_has_pretrained = set()  # set of model names that have pretrained weight url present
__all__ = ['list_models', 'is_model', 'model_entrypoint', 'list_modules', 'is_model_in_modules']

def register_model(fn):
    # lookup containing module
    mod = sys.modules[fn.__module__]
    module_name_split = fn.__module__.split('.')
    module_name = module_name_split[-1] if len(module_name_split) else ''

    # add model to __all__ in module
    model_name = fn.__name__
    if hasattr(mod, '__all__'):
        mod.__all__.append(model_name)
    else:
        mod.__all__ = [model_name]

    # add entries to registry dict/sets
    _model_entrypoints[model_name] = fn
    _model_to_module[model_name] = module_name
    _module_to_models[module_name].add(model_name)
    has_pretrained = False  # check if model has a pretrained url to allow filtering on this
    if hasattr(mod, 'default_cfgs') and model_name in mod.default_cfgs:
        # this will catch all models that have entrypoint matching cfg key, but miss any aliasing
        # entrypoints or non-matching combos
        has_pretrained = 'url' in mod.default_cfgs[model_name] and 'http' in mod.default_cfgs[model_name]['url']
    if has_pretrained:
        _model_has_pretrained.add(model_name)
    return fn

IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)
IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)

def _calculate_fan_in_and_fan_out(tensor):
    dimensions = tensor.ndim()
    if dimensions < 2:
        raise ValueError("Fan in and fan out can not be computed for tensor with fewer than 2 dimensions")

    num_input_fmaps = tensor.shape[1]
    num_output_fmaps = tensor.shape[0]
    receptive_field_size = 1
    if tensor.ndim() > 2:
        receptive_field_size = tensor[0][0].numel()
    fan_in = num_input_fmaps * receptive_field_size
    fan_out = num_output_fmaps * receptive_field_size

    return fan_in, fan_out


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with jt.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        # tensor.uniform_(2 * l - 1, 2 * u - 1)
        jt.nn.init.uniform_(tensor, 2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        # tensor.erfinv_()
        tensor = jt.array(erfinv(tensor.numpy()))

        # Transform to proper mean, std
        tensor = tensor * (std * math.sqrt(2.))
        tensor.add(mean)

        # Clamp to ensure it's in the proper range
        jt.clamp(tensor, min_v=a, max_v=b)
    return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (jt.Var, float, float, float, float) -> jt.Var
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def variance_scaling_(tensor, scale=1.0, mode='fan_in', distribution='normal'):
    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)
    if mode == 'fan_in':
        denom = fan_in
    elif mode == 'fan_out':
        denom = fan_out
    elif mode == 'fan_avg':
        denom = (fan_in + fan_out) / 2

    variance = scale / denom

    if distribution == "truncated_normal":
        # constant is stddev of standard normal truncated to (-2, 2)
        trunc_normal_(tensor, std=math.sqrt(variance) / .87962566103423978)
    elif distribution == "normal":
        # tensor.normal_(std=math.sqrt(variance))
        jt.nn.init.gauss_(tensor, std=math.sqrt(variance))
    elif distribution == "uniform":
        bound = math.sqrt(3 * variance)
        # tensor.uniform_(-bound, bound)
        jt.nn.init.uniform_(tensor, -bound, bound)
    else:
        raise ValueError(f"invalid distribution {distribution}")

def _ntuple(n):
    def parse(x):
        if isinstance(x, collections.abc.Iterable):
            return x
        return tuple(repeat(x, n))
    return parse


to_1tuple = _ntuple(1)
to_2tuple = _ntuple(2)
to_3tuple = _ntuple(3)
to_4tuple = _ntuple(4)
to_ntuple = _ntuple


def make_divisible(v, divisor=8, min_value=None, round_limit=.9):
    min_value = min_value or divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    # Make sure that round down does not go down by more than 10%.
    if new_v < round_limit * v:
        new_v += divisor
    return new_v

def drop_path(x, drop_prob: float = 0., training: bool = False):
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + jt.rand(shape, dtype=x.dtype)
    random_tensor.floor()  # binarize
    output = x / keep_prob * random_tensor
    return output

class GroupConvolution(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):
        super(GroupConvolution, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        if isinstance(kernel_size, int):
            self.kernel_size = (kernel_size, kernel_size)
        self.stride = stride
        self.padding = padding
        self.dilation = dilation
        self.groups = groups
        self.use_bias = bias

        # 计算每个分组的输入和输出通道数
        self.group_in_channels = in_channels // groups
        self.group_out_channels = out_channels // groups

        # 创建卷积核权重参数
        self.weight = jt.Var(jt.randn((out_channels, self.group_in_channels, *self.kernel_size))).start_grad()
        init.gauss_(self.weight, mean=0.0, std=0.02)

        if bias:
            self.bias = jt.Var(jt.zeros((out_channels,))).start_grad()
            init.constant_(self.bias, 0)

    def execute(self, x):
        assert x.shape[1] == self.in_channels, "输入通道数必须与定义的一致"
        
        # 直接使用带有分组参数的卷积函数，无需手动切分
        out = nn.conv2d(x, self.weight, None, self.stride, self.padding, self.dilation, self.groups)

        if self.use_bias:
            out += self.bias.reshape(1, self.out_channels, 1, 1)

        return out

class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """

    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def execute(self, x):
        return drop_path(x, self.drop_prob, self.is_training())
    
class MultiPrototypes(nn.Module):
    def __init__(self, output_dim, nmb_prototypes):
        super().__init__()
        self.nmb_heads = len(nmb_prototypes)
        for i, k in enumerate(nmb_prototypes):
            setattr(self, 'prototypes' + str(i),
                            nn.Linear(output_dim, k, bias=False))

    def execute(self, x):
        out = []
        for i in range(self.nmb_heads):
            out.append(getattr(self, 'prototypes' + str(i))(x))
        return out

class Mulpixelattn(nn.Module):
    def __init__(self, channels):
        super().__init__()
        hidden_mlp = channels
        self.atten = nn.Sequential(
            nn.Conv2d(channels, hidden_mlp, 1),
            nn.BatchNorm2d(hidden_mlp),
            nn.ReLU(),
            nn.Conv2d(hidden_mlp, channels, 1),
            nn.BatchNorm2d(channels, affine=True),
        )
        self.threshold = jt.zeros((1, channels, 1, 1))  #没有梯度
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))

    def execute(self, x):
        x = self.atten(x)
        x = x + self.threshold
        att = jt.sigmoid(x)   #64,
        return att

class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU(), drop=0.):
        super().__init__()  
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Conv2d(in_features, hidden_features, 1)
        self.dwconv = GroupConvolution(hidden_features, hidden_features, 3, stride=1, padding=1, groups=hidden_features, bias=True)
        self.act = act_layer
        self.fc2 = nn.Conv2d(hidden_features, out_features, 1)
        self.drop = nn.Dropout(drop)
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight,
                                    mode='fan_out',
                                    nonlinearity='relu')

    def execute(self, x):
        x = self.fc1(x)
        x = self.dwconv(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)

        return x


class AttentionModule(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.conv0 = GroupConvolution(dim, dim, 5, padding=2, groups=dim)
        self.conv_spatial = GroupConvolution(dim, dim, 7, stride=1, padding=9, groups=dim, dilation=3)
        self.conv1 = nn.Conv2d(dim, dim, 1)

    def execute(self, x):      
        out = x
        attn = self.conv1(self.conv_spatial(self.conv0(x)))
        out = out * attn
        return out


class SpatialAttention(nn.Module):
    def __init__(self, d_model):
        super().__init__()

        self.proj_1 = nn.Conv2d(d_model, d_model, 1)
        self.activation = nn.GELU()
        self.spatial_gating_unit = AttentionModule(d_model)
        self.proj_2 = nn.Conv2d(d_model, d_model, 1)

    def execute(self, x):
        shorcut = x
        x = self.proj_1(x)
        x = self.activation(x)
        x = self.spatial_gating_unit(x)
        x = self.proj_2(x)
        x += shorcut
        return x


class Block(nn.Module):
    def __init__(self, dim, mlp_ratio=4., drop=0.,drop_path=0., act_layer=nn.GELU()):
        super().__init__()
        self.norm1 = nn.BatchNorm2d(dim)
        self.attn = SpatialAttention(dim)
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()

        self.norm2 = nn.BatchNorm2d(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)
        layer_scale_init_value = 1e-2            
        
        # self.layer_scale_1 = nn.Parameter(layer_scale_init_value * jt.ones((dim)), requires_grad=True)
        # self.layer_scale_2 = nn.Parameter(layer_scale_init_value * jt.ones((dim)), requires_grad=True)
        self.layer_scale_1 = jt.Var(
            layer_scale_init_value * jt.ones((dim))).start_grad()
        self.layer_scale_2 = jt.Var(
            layer_scale_init_value * jt.ones((dim))).start_grad()

        
        self.apply(self._init_weights)

    # def start_grad(x):
    #     return x._update(x)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight,
                                    mode='fan_out',
                                    nonlinearity='relu')

    def execute(self, x):
        x = x + self.drop_path(self.layer_scale_1.unsqueeze(-1).unsqueeze(-1) * self.attn(self.norm1(x)))
        x = x + self.drop_path(self.layer_scale_2.unsqueeze(-1).unsqueeze(-1) * self.mlp(self.norm2(x)))

        return x


class OverlapPatchEmbed(nn.Module):
    """ Image to Patch Embedding
    """

    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)

        self.img_size = img_size
        self.patch_size = patch_size
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,
                              padding=(patch_size[0] // 2, patch_size[1] // 2))
        self.norm = nn.BatchNorm2d(embed_dim)

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight,
                                    mode='fan_out',
                                    nonlinearity='relu')

            if m.bias is not None:
                # m.bias.data.zero_()
                init.constant_(m.bias)

    def execute(self, x):
        x = self.proj(x)
        _, _, H, W = x.shape
        x = self.norm(x)        
        return x, H, W


class VAN(nn.Module):
    def __init__(self, img_size=224, crop_size=96, in_chans=3, num_classes=51, embed_dims=[64, 128, 256, 512],
                mlp_ratios=[4, 4, 4, 4], drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,
                depths=[3, 4, 6, 3], num_stages=4, flag=False, 
                output_dim=0, hidden_mlp=0, nmb_prototypes=0, eval_mode=False,train_mode="finetune", shallow=None, normalize=False,
                in_channels=[128,320,512], in_index=[1,2,3], channels=1024, dropout_ratio=0.1, norm_cfg=dict(type='GN', num_groups=32), 
                align_corners=False, ham_channels=1024, ham_kwargs=dict()):
        super().__init__()
        assert train_mode in ['pretrain', 'pixelattn', 'finetune'], train_mode
        if flag == False:
            self.num_classes = num_classes
        self.depths = depths
        self.num_stages = num_stages
        self.in_index = in_index
        self.img_size = img_size
        dpr = [x.item() for x in jt.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule
        cur = 0
        
        for i in range(num_stages):
            patch_embed = OverlapPatchEmbed(img_size=img_size if i == 0 else img_size // (2 ** (i + 1)),
                                            patch_size=7 if i == 0 else 3,
                                            stride=4 if i == 0 else 2,
                                            in_chans=in_chans if i == 0 else embed_dims[i - 1],
                                            embed_dim=embed_dims[i])
            
            block = nn.Sequential(*[Block(
                dim=embed_dims[i], mlp_ratio=mlp_ratios[i], drop=drop_rate, drop_path=dpr[cur + j])
                for j in range(depths[i])])
            
            norm = norm_layer(embed_dims[i])
            cur += depths[i]

            setattr(self, f"patch_embed{i + 1}", patch_embed)
            setattr(self, f"block{i + 1}", block)
            setattr(self, f"norm{i + 1}", norm)

        #swav的设置
        self.eval_mode = eval_mode
        self.train_mode = train_mode
        self.shallow = shallow 
        if isinstance(self.shallow, int):
            self.shallow = [self.shallow]
        self.avgpool = nn.AdaptiveAvgPool2d((1,1))
        self.l2norm = normalize
        
        #projection head
        if output_dim == 0:
            self.projection_head = None
            if self.train_mode == "pretrain":
                self.projection_head_shallow = None
        elif hidden_mlp == 0:
            self.projection_head = nn.Linear(embed_dims[3],output_dim)
            if self.train_mode == "pretrain":
                self.projection_head_shallow = nn.ModuleList()
                if self.shallow is not None:
                    for stage in shallow:
                        assert stage < 4
                        self.projection_head_shallow.add_module(
                        f'projection_head_shallow{stage}',
                            nn.Linear(embed_dims[3],
                                      output_dim))
        else:
            mlps = [
                nn.Linear(embed_dims[3], hidden_mlp),
                nn.BatchNorm1d(hidden_mlp),
                nn.ReLU(),
                nn.Linear(hidden_mlp, output_dim),
                nn.BatchNorm1d(output_dim, affine=False)                
            ]
            self.projection_head = nn.Sequential(*mlps)
            if self.train_mode == "pretrain":
                self.projection_head_shallow = nn.ModuleList()
                if self.shallow is not None:
                    for stage in shallow:
                        assert stage < 4
                        self.projection_head_shallow.add_module(
                            f'projection_head_shallow{stage}',
                            nn.Sequential(
                                nn.Linear(embed_dims[stage-1], hidden_mlp), 
                                nn.BatchNorm1d(hidden_mlp),
                                nn.ReLU(),
                                nn.Linear(hidden_mlp, output_dim),
                                nn.BatchNorm1d(output_dim, affine=False)))
                       
        if self.train_mode == 'pretrain': #projection_head_pixel_shallow（每一次两个linear，最后映射到64维度）执行三次。
            self.projection_head_pixel_shallow = nn.ModuleList()
            if self.shallow is not None:
                for stage in shallow:
                    assert stage < 4
                    self.projection_head_pixel_shallow.add_module(    
                        f'projection_head_pixel{stage}',
                        nn.Sequential(
                            nn.Conv2d(
                                embed_dims[stage-1],
                                hidden_mlp,
                                kernel_size=1,
                                bias=False,
                            ),
                            nn.BatchNorm2d(hidden_mlp),
                            nn.ReLU(),
                            nn.Conv2d(hidden_mlp,
                                      hidden_mlp,
                                      kernel_size=1,
                                      bias=False),
                            nn.BatchNorm2d(hidden_mlp),
                            nn.ReLU(),
                            nn.Conv2d(
                                hidden_mlp,
                                embed_dims[stage-1],
                                kernel_size=1,
                                bias=False,
                            ),
                            nn.BatchNorm2d(embed_dims[stage-1],
                                           affine=False),
                        ))
            
            self.projection_head_pixel = nn.Sequential(
                nn.Conv2d(embed_dims[3],
                          hidden_mlp,
                          kernel_size=1,
                          bias=False),
                nn.BatchNorm2d(hidden_mlp),
                nn.ReLU(),
                nn.Conv2d(hidden_mlp, hidden_mlp, kernel_size=1, bias=False),
                nn.BatchNorm2d(hidden_mlp),
                nn.ReLU(),
                nn.Conv2d(hidden_mlp, output_dim, kernel_size=1, bias=False),
                nn.BatchNorm2d(output_dim, affine=False),
            )
            self.predictor_head_pixel = nn.Sequential(  #最后还是128
                nn.Conv2d(output_dim, output_dim, 1, bias=False),
                nn.BatchNorm2d(output_dim),
                nn.ReLU(),
                nn.Conv2d(output_dim, output_dim, 1),
            )
        
        #prototype layer
        self.prototypes = None
        if isinstance(nmb_prototypes, list):
            self.prototypes = MultiPrototypes(output_dim, nmb_prototypes)
        elif nmb_prototypes > 0:
            self.prototypes = nn.Linear(output_dim, nmb_prototypes, bias=False)  
        
        if train_mode == "pixelattn":
            self.fbg = Mulpixelattn(sum([embed_dims[i] for i in in_index]))  #0-1的mask（过两层线形层），这个mask是128维度的
        elif train_mode == 'finetune':
            self.last_layer = LightHamHead(in_channels=in_channels,
                                           in_index=in_index,
                                           channels=channels,
                                           dropout_ratio=dropout_ratio,
                                           num_classes=num_classes,
                                           norm_cfg=norm_cfg,
                                           align_corners=align_corners,
                                           ham_channels=ham_channels,
                                           ham_kwargs=ham_kwargs) 

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight,
                                mode='fan_out',
                                nonlinearity='relu')
            if m.bias is not None:
                # m.bias.data.zero_()
                init.constant_(m.bias)

    def freeze_patch_emb(self):
        self.patch_embed1.requires_grad = False

    # @torch.jit.ignore
    def no_weight_decay(self):
        return {'pos_embed1', 'pos_embed2', 'pos_embed3', 'pos_embed4', 'cls_token'}  # has pos_embed may be better

    def execute_backbone(self, x, avgpool=True):
        B = x.shape[0]
        outs = []
        
        for i in range(self.num_stages):
            patch_embed = getattr(self, f"patch_embed{i + 1}")
            block = getattr(self, f"block{i + 1}")
            norm = getattr(self, f"norm{i + 1}")
            x, H, W = patch_embed(x)
            #for blk in block:
            x = block(x)
            x_tmp = jt.transpose(x.flatten(2),0,2,1)
            x_tmp = norm(x_tmp)
            x = x_tmp.reshape(B, H, W, -1).permute(0, 3, 1, 2)
            outs.append(x)
        
        if avgpool:
            output = self.avgpool(outs[-1])
            output = jt.flatten(output,1)
            return output
        return outs
    
    def execute_head(self, x):
        if self.projection_head is not None:
            x = self.projection_head(x)
        
        if self.l2norm:
            x = jt.normalize(x, dim=1, p=2)
            
        if self.prototypes is not None:
            return x, self.prototypes(x)
        return x
    
    def execute_head_shallow(self, x, stage):
        if (self.projection_head_shallow is not None
                and f'projection_head_shallow{stage}'
                in self.projection_head_shallow.keys()):
            x = self.projection_head_shallow.layers[
                f'projection_head_shallow{stage}'](x)

        if self.l2norm:
            x = jt.normalize(x, dim=1, p=2)

        if self.prototypes is not None:
            return x, nn.matmul_transpose(x, self.prototypes.weight.detach()) 
        return x
    
    def execute_head_pixel(self, x, gridq, gridk):
        if self.projection_head_pixel is not None:
            x = self.projection_head_pixel(x)

        # grid sample 28 x 28
        grid = jt.concat([gridq, gridk], dim=0)
        x = nn.grid_sample(x, grid, align_corners=False, mode='bilinear')

        return x, self.predictor_head_pixel(x)
    
    def execute(self, inputs, gridq=None, gridk=None, mode='train'):
        if mode == 'cluster':
            output = self.inference_cluster(inputs)    #对输入进行聚类
            return output
        elif mode == 'inference_pixel_attention':
            return self.inference_pixel_attention(inputs)

        if self.train_mode == "finetune":
            out = self.execute_backbone(inputs) #返回每个像素的特征
            return self.last_layer(out)         #最后投影到类别        
        
        if not isinstance(inputs, list):
            inputs = [inputs]
        idx_crops, last_size = [0], inputs[0].shape[-1]
        for sample in [inp.shape[-1] for inp in inputs]:
            if sample == last_size:
                idx_crops[-1] += 1
            else:
                idx_crops.append(idx_crops[-1] + 1)
        
        start_idx = 0
        for end_idx in idx_crops:
            _out = self.execute_backbone(jt.concat(
                inputs[start_idx:end_idx]), avgpool=self.train_mode != 'pretrain')
            if start_idx == 0:
                if self.train_mode == 'pixelattn':
                    _output = self.execute_pixel_attention(_out)   
                elif self.train_mode == 'pretrain':
                    (embedding_deep_pixel, output_deep_pixel, ) = self.execute_head_pixel(_out[-1], gridq, gridk)   #16,128,7,7 
                    _stages = _out[:self.shallow[0]]

                    if self.shallow is not None:
                        output_c = []
                        for i, stage in enumerate(self.shallow):
                            _c = _stages[stage-1]
                            _out_c = self.projection_head_pixel_shallow.layers[    #每一个中间层的特征拿出来，然后接上一个投影头
                                f'projection_head_pixel{stage}'](_c)

                            _out_c = self.avgpool(_out_c)                          #对结果取平均，最后只有一个image的feature
                            _out_c = jt.flatten(_out_c, 1)
                            output_c.append(_out_c)                                #作为output C （统一为64维）
                            
                    _output = self.avgpool(_out[-1])                                      #再将最后一层取平均作为output=_out (图像的最后一层维度)
                    _output = jt.flatten(_output, 1)                          #16,512
                output = _output

            else:
                if self.train_mode == 'pixelattn':
                    _output = self.execute_pixel_attention(_out)  
                elif self.train_mode == 'pretrain':
                    _output = self.avgpool(_out[-1])                                      #最后一层拿出来取平均，得到global的结果。
                    _output = jt.flatten(_output, 1)  

                output = jt.concat((output, _output))
            start_idx = end_idx     
        

        embedding, output = self.execute_head(output)     #输出64,128 和 64，500；输入64.512
        if self.shallow is not None:                                               #对浅层的global特征也做非线性处理，再做聚类，输出聚类前的特征和聚类结果。
            for i, stage in enumerate(self.shallow):
                embedding_c_, output_c_ = self.execute_head_shallow(output_c[i], stage=stage)          #输出16，128和16，500
                embedding = jt.concat((embedding, embedding_c_))
                output = jt.concat((output, output_c_))

        if self.train_mode == 'pixelattn':
            return embedding, output                                               #输出聚类的全局特征，和聚类结果。
       
        elif self.train_mode == 'pretrain':
            return embedding, output, embedding_deep_pixel, output_deep_pixel      #输出所有图像的全局特征（global图像和local图像），所有聚类结果，global图像的local特征，global图像的local的投影特征
           
        return embedding, output
                    
    def execute_pixel_attention(self, out, threshold=0.):  #执行pixel-attention的地方
        out = [
            resize(level,
                   size=out[0].shape[2:],
                   mode='bilinear',
                   align_corners=self.align_corners) for level in out
        ]
        out = jt.concat(out,dim=1)
        out = jt.normalize(out, dim=1, p=2)                  #归一化
        fg = self.fbg(out)                                   #前背景二值化
        if threshold is not None:                            #没有前背景的阈值
            fg[fg < threshold] = 0

        out = out * fg                                       #得到前景的特征
        out = self.avgpool(out)
        out = jt.flatten(out, 1)                             #前景的特征avg pool一下，输出

        return out
            
    def inference_cluster(self, x, threshold=0.):
        out = self.execute_backbone(x)
        out = [
            resize(level,
                   size=out[0].shape[2:],
                   mode='bilinear',
                   align_corners=self.align_corners) for level in out
        ]
        out = jt.concat(out,dim=1)
        nout = jt.normalize(out, dim=1, p=2)               #归一化
        fg = self.fbg(nout)                                #预测前背景
        if threshold is not None:                          #还是没有阈值，外面没传进来。
            fg[fg < threshold] = 0

        out = out * fg                                     
        out = self.avgpool(out)
        out = jt.flatten(out, 1)                           #输出前景的global结果

        return out

    def inference_pixel_attention(self, x):
        out = self.execute_backbone(x)            #提取出最后的pixel特征
        out = [
            resize(level,
                   size=out[0].shape[2:],
                   mode='bilinear',
                   align_corners=self.align_corners) for level in out
        ]
        out = jt.concat(out,dim=1)
        out_ = jt.normalize(out, dim=1, p=2)          
        fg = self.fbg(out_)                       #对pixel特征归一化的结果生成前景
        fg = fg.mean(dim=1, keepdims=True)        #前景对所有128维度的通道做一个mean，得到一个1维的H，W的mask

        return out, fg

def _conv_filter(state_dict, patch_size=16):
    """ convert patch embedding weight from manual patchify + linear proj to conv"""
    out_dict = {}
    for k, v in state_dict.items():
        if 'patch_embed.proj.weight' in k:
            v = v.reshape((v.shape[0], 3, patch_size, patch_size))
        out_dict[k] = v

    return out_dict

@register_model
def van_tiny(**kwargs):
    model = VAN(
        embed_dims=[32, 64, 160, 256], mlp_ratios=[8, 8, 4, 4], 
        norm_layer=nn.LayerNorm, depths=[3, 3, 5, 2], drop_rate=0.0, drop_path_rate=0.1,
        in_channels=[64, 160, 256], in_index=[1, 2, 3], channels=256, dropout_ratio=0.1, 
        ham_channels=256, ham_kwargs=dict(MD_R=16), **kwargs)

    return model

@register_model
def van_small(**kwargs):
    model = VAN(
        embed_dims=[64, 128, 320, 512], mlp_ratios=[8, 8, 4, 4],
        norm_layer=nn.LayerNorm, depths=[2, 2, 4, 2], drop_rate=0.0, drop_path_rate=0.1,
        in_channels=[128, 320, 512], in_index=[1, 2, 3], channels=256, dropout_ratio=0.1,
        ham_channels=256, ham_kwargs=dict(MD_R=16), **kwargs)

    return model

@register_model
def van_base(**kwargs):
    model = VAN(
        embed_dims=[64, 128, 320, 512], mlp_ratios=[8, 8, 4, 4],
        norm_layer=nn.LayerNorm, depths=[3, 3, 12, 3], drop_rate=0.0, drop_path_rate=0.2,
        
        in_channels=[128, 320, 512], in_index=[1, 2, 3], channels=512, dropout_ratio=0.1,
        ham_channels=512, **kwargs)

    return model

@register_model
def van_large(**kwargs):
    model = VAN(
        embed_dims=[64, 128, 320, 512], mlp_ratios=[8, 8, 4, 4], 
        norm_layer=nn.LayerNorm, depths=[3, 5, 27, 3], drop_rate=0.0, drop_path_rate=0.3,
        in_channels=[128, 320, 512], in_index=[1, 2, 3], channels=1024, dropout_ratio=0.1,
        ham_channels=1024, **kwargs)

    return model